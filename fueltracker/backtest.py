"""Backtesting functionality for Fuel Tracker."""import pandas as pdimport numpy as npfrom pathlib import Pathfrom typing import Dict, Any, Optionalfrom datetime import datetimeimport argparseimport sysfrom .io_parquet import read_panel, get_panel_infofrom .models.baseline import SeasonalNaive, rolling_backtest as baseline_rolling_backtestfrom .models.stl_ets import STLETS, rolling_backtest as stl_ets_rolling_backtestfrom .models.sarimax import SARIMAX, rolling_backtest as sarimax_rolling_backtestfrom .lineage import start_batchfrom .logging_utils import get_loggerfrom .config import OUTPUTS_DIR, METRICS_FILElogger = get_logger(__name__)def get_frozen_subpanel(    panel_path: Path,    asof_ts: Optional[datetime] = None) -> pd.DataFrame:    """    Read latest frozen sub-panel up to a given asof_ts.    Args:        panel_path: Path to the panel file        asof_ts: Cutoff timestamp (default: latest batch)    Returns:        DataFrame with frozen sub-panel    """    if not panel_path.exists():        logger.warning("Panel file does not exist", extra={"path": str(panel_path)})        return pd.DataFrame()    # Read the full panel    panel_df = read_panel(panel_path)    if panel_df.empty:        logger.warning("Panel is empty")        return panel_df    # If no asof_ts specified, use the latest batch timestamp    if asof_ts is None:        latest_batch_ts = panel_df['asof_ts'].max()        asof_ts = latest_batch_ts        logger.info("Using latest batch timestamp", extra={"asof_ts": asof_ts.isoformat()})    # Filter to data up to the cutoff timestamp    # For backtesting, we want to filter by the data period, not the batch timestamp    # Convert asof_ts to date for comparison with period    if asof_ts:        cutoff_date = asof_ts.date()        frozen_panel = panel_df[panel_df['period'] <= cutoff_date].copy()        logger.info("Filtered by period cutoff", extra={"cutoff_date": cutoff_date.isoformat()})    else:        frozen_panel = panel_df.copy()    # Sort by period and remove duplicates (keep latest for each period)    frozen_panel = frozen_panel.sort_values(['period', 'asof_ts']).drop_duplicates(        subset=['period'], keep='last'    ).sort_values('period')    logger.info("Frozen sub-panel created", extra={        "total_rows": len(panel_df),        "frozen_rows": len(frozen_panel),        "asof_ts": asof_ts.isoformat(),        "date_range": f"{frozen_panel['period'].min()} to {frozen_panel['period'].max()}" if not frozen_panel.empty else "empty"    })    return frozen_paneldef run_model_backtest(    panel_df: pd.DataFrame,    horizon: int,    last_n_months: int,    model: str = "baseline") -> pd.DataFrame:    """    Run baseline backtest on panel data.    Args:        panel_df: Panel DataFrame        horizon: Forecast horizon        last_n_months: Number of months to use for backtesting    Returns:        DataFrame with backtest results    """    if panel_df.empty:        logger.warning("Panel is empty, cannot run backtest")        return pd.DataFrame()    # Prepare time series data    panel_df = panel_df.sort_values('period')    # Create time series with period as index    y = panel_df.set_index('period')['value_mmcf']    logger.info(f"Running {model} backtest", extra={        "data_length": len(y),        "horizon": horizon,        "last_n_months": last_n_months,        "panel_rows": len(panel_df),        "model": model    })    # Run rolling backtest based on model type    if model == "baseline":        backtest_results = baseline_rolling_backtest(            y=y,            horizon=horizon,            last_n_months=last_n_months,            model_class=SeasonalNaive,            period=12        )    elif model == "stl_ets":        backtest_results = stl_ets_rolling_backtest(            y=y,            horizon=horizon,            last_n_months=last_n_months,            model_class=STLETS,            period=12        )    elif model == "sarimax":        backtest_results = sarimax_rolling_backtest(            y=y,            horizon=horizon,            last_n_months=last_n_months,            model_class=SARIMAX,            order=(1, 1, 1),            seasonal_order=(1, 1, 1, 12)        )    else:        raise ValueError(f"Unsupported model: {model}")    return backtest_resultsdef write_metrics(    backtest_results: pd.DataFrame,    batch_meta: Any,    metrics_path: Path,    model: str = "baseline") -> None:    """    Write backtest metrics to CSV file.    Args:        backtest_results: DataFrame with backtest results        batch_meta: Batch metadata        metrics_path: Path to metrics file    """    if backtest_results.empty:        logger.warning("No backtest results to write")        return    # Add batch information    results_with_batch = backtest_results.copy()    results_with_batch['batch_id'] = str(batch_meta.batch_id)    results_with_batch['asof_ts'] = batch_meta.asof_ts.isoformat()    # Set model name based on model type    if model == "baseline":        model_name = "seasonal_naive"    elif model == "stl_ets":        model_name = "stl_ets"    elif model == "sarimax":        model_name = "sarimax"    else:        model_name = model    results_with_batch['model'] = model_name    # Ensure directory exists    metrics_path.parent.mkdir(parents=True, exist_ok=True)    # Append to existing file or create new one    if metrics_path.exists():        existing_metrics = pd.read_csv(metrics_path)        combined_metrics = pd.concat([existing_metrics, results_with_batch], ignore_index=True)        logger.info("Appending to existing metrics file", extra={            "existing_rows": len(existing_metrics),            "new_rows": len(results_with_batch),            "total_rows": len(combined_metrics)        })    else:        combined_metrics = results_with_batch        logger.info("Creating new metrics file", extra={"rows": len(combined_metrics)})    # Write combined metrics    combined_metrics.to_csv(metrics_path, index=False)    logger.info("Metrics written successfully", extra={        "path": str(metrics_path),        "total_rows": len(combined_metrics)    })def run_backtest_pipeline(    asof_ts: Optional[str] = None,    model: str = "baseline",    horizon: int = 12,    last_n_months: int = 60) -> Dict[str, Any]:    """    Run complete backtest pipeline.    Args:        asof_ts: Cutoff timestamp string (ISO format)        model: Model to use (currently only "baseline" supported)        horizon: Forecast horizon        last_n_months: Number of months for backtesting    Returns:        Dictionary with pipeline results    """    logger.info("Starting backtest pipeline", extra={        "asof_ts": asof_ts,        "model": model,        "horizon": horizon,        "last_n_months": last_n_months    })    # Start batch for this backtest run    batch = start_batch(source="BACKTEST", notes=f"Backtest run for {model} model")    try:        # Parse asof_ts if provided        parsed_asof_ts = None        if asof_ts:            try:                parsed_asof_ts = pd.to_datetime(asof_ts)                logger.info("Parsed asof_ts", extra={"asof_ts": parsed_asof_ts.isoformat()})            except ValueError as e:                raise ValueError(f"Invalid asof_ts format: {asof_ts}. Expected ISO format.")        # Get panel path        panel_path = OUTPUTS_DIR / "panel_monthly.parquet"        # Get frozen sub-panel        frozen_panel = get_frozen_subpanel(panel_path, parsed_asof_ts)        if frozen_panel.empty:            return {                "error": "No data available for backtesting",                "batch_id": str(batch.batch_id)            }        # Run model backtest        backtest_results = run_model_backtest(            frozen_panel,            horizon=horizon,            last_n_months=last_n_months,            model=model        )        if backtest_results.empty:            return {                "error": "Backtest produced no results",                "batch_id": str(batch.batch_id)            }        # Write metrics        metrics_path = OUTPUTS_DIR / METRICS_FILE        write_metrics(backtest_results, batch, metrics_path, model)        # Calculate summary statistics        summary_stats = {            'mae_mean': backtest_results['mae'].mean(),            'mae_std': backtest_results['mae'].std(),            'smape_mean': backtest_results['smape'].mean(),            'smape_std': backtest_results['smape'].std(),            'splits': len(backtest_results)        }        results = {            "batch_id": str(batch.batch_id),            "asof_ts": batch.asof_ts.isoformat(),            "model": model,            "data_rows": len(frozen_panel),            "backtest_splits": len(backtest_results),            "horizon": horizon,            "last_n_months": last_n_months,            "summary_stats": summary_stats,            "metrics_file": str(metrics_path)        }        logger.info("Backtest pipeline completed successfully", extra=results)        return results    except Exception as e:        logger.error("Backtest pipeline failed", extra={            "error": str(e),            "batch_id": str(batch.batch_id)        })        raisedef main():    """Main entry point for backtest pipeline."""    parser = argparse.ArgumentParser(description="Run backtest pipeline")    parser.add_argument(        "--asof",        type=str,        help="Cutoff timestamp in ISO format (e.g., YYYY-MM-DDTHH:MM:SSZ)"    )    parser.add_argument(        "--model",        type=str,        default="baseline",        choices=["baseline", "stl_ets", "sarimax"],        help="Model to use for backtesting"    )    parser.add_argument(        "--horizon",        type=int,        default=12,        help="Forecast horizon"    )    parser.add_argument(        "--last-n-months",        type=int,        default=60,        help="Number of months to use for backtesting"    )    args = parser.parse_args()    try:        results = run_backtest_pipeline(            asof_ts=args.asof,            model=args.model,            horizon=args.horizon,            last_n_months=args.last_n_months        )        if "error" in results:            print(f"❌ Backtest failed: {results['error']}")            sys.exit(1)        # Print results        print("\n🚀 Backtest Results:")        print(f"   Batch ID: {results['batch_id']}")        print(f"   Model: {results['model']}")        print(f"   Data rows: {results['data_rows']}")        print(f"   Backtest splits: {results['backtest_splits']}")        print(f"   Horizon: {results['horizon']}")        print(f"   Last N months: {results['last_n_months']}")        if results.get("summary_stats"):            stats = results["summary_stats"]            print(f"   MAE: {stats['mae_mean']:.2f} ± {stats['mae_std']:.2f}")            print(f"   sMAPE: {stats['smape_mean']:.2f} ± {stats['smape_std']:.2f}")        print(f"   Metrics file: {results['metrics_file']}")        print("\n✅ Backtest completed successfully!")        sys.exit(0)    except Exception as e:        print(f"❌ Backtest failed: {e}")        sys.exit(1)if __name__ == "__main__":    main()