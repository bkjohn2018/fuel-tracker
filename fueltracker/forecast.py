"""Forecasting functionality for Fuel Tracker."""import pandas as pdimport numpy as npfrom pathlib import Pathfrom typing import Dict, Any, Optional, Tuplefrom datetime import datetimeimport argparseimport sysfrom .io_parquet import read_panel, get_panel_infofrom .models.baseline import SeasonalNaive, generate_baseline_forecastfrom .models.stl_ets import STLETS, generate_stl_ets_forecastfrom .models.sarimax import SARIMAX, generate_sarimax_forecastfrom .lineage import start_batchfrom .logging_utils import get_loggerfrom .config import OUTPUTS_DIR, FORECAST_FILElogger = get_logger(__name__)def select_winning_model(metrics_path: Path) -> str:    """    Select the winning model based on backtest performance.    Args:        metrics_path: Path to metrics CSV file    Returns:        Name of the winning model    """    if not metrics_path.exists():        logger.info("No metrics file found, using baseline model as default")        return "baseline"    try:        metrics_df = pd.read_csv(metrics_path)        if metrics_df.empty:            logger.info("Metrics file is empty, using baseline model as default")            return "baseline"        # For now, always select baseline (stub selector)        # TODO: Implement proper model selection based on performance        logger.info("Using baseline model as winner (stub selector)")        return "baseline"        # Future implementation could look like:        # model_performance = metrics_df.groupby('model').agg({        #     'mae': 'mean',        #     'smape': 'mean'        # }).reset_index()        # winning_model = model_performance.loc[model_performance['mae'].idxmin(), 'model']        # return winning_model    except Exception as e:        logger.warning(f"Error reading metrics file: {e}, using baseline model")        return "baseline"def generate_forecast_with_pi(    panel_df: pd.DataFrame,    model: str,    horizon: int = 12) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:    """    Generate forecast with prediction intervals.    Args:        panel_df: Panel DataFrame        model: Model to use for forecasting        horizon: Forecast horizon    Returns:        Tuple of (forecasts, pi_lower, pi_upper)    """    if panel_df.empty:        raise ValueError("Panel is empty, cannot generate forecast")    # Prepare time series data    panel_df = panel_df.sort_values('period')    y = panel_df.set_index('period')['value_mmcf']    logger.info(f"Generating {horizon}-month forecast using {model} model", extra={        "data_length": len(y),        "horizon": horizon,        "model": model    })    # Generate forecast based on model type    if model == "baseline":        forecasts, model_info = generate_baseline_forecast(y, horizon, SeasonalNaive, period=12)    elif model == "stl_ets":        forecasts, model_info = generate_stl_ets_forecast(y, horizon, period=12)    elif model == "sarimax":        forecasts, model_info = generate_sarimax_forecast(y, horizon, order=(1, 1, 1), seasonal_order=(1, 1, 1, 12))    else:        raise ValueError(f"Unsupported model: {model}")    # Calculate prediction intervals using historical MAE    # Simple approach: use historical MAE as half-width for naive PI bands    historical_mae = _calculate_historical_mae(panel_df, model)    # Create prediction intervals    pi_half_width = historical_mae    pi_lower = np.maximum(0, forecasts - pi_half_width)  # Ensure non-negative    pi_upper = forecasts + pi_half_width    logger.info("Forecast generated with prediction intervals", extra={        "forecast_mean": float(np.mean(forecasts)),        "pi_half_width": float(pi_half_width),        "pi_lower_mean": float(np.mean(pi_lower)),        "pi_upper_mean": float(np.mean(pi_upper))    })    return forecasts, pi_lower, pi_upperdef _calculate_historical_mae(panel_df: pd.DataFrame, model: str) -> float:    """    Calculate historical MAE for prediction interval calculation.    Args:        panel_df: Panel DataFrame        model: Model name for context    Returns:        Historical MAE value    """    # For now, use a simple approach: calculate MAE from the most recent data    # This could be enhanced to use actual backtest results from metrics file    if len(panel_df) < 2:        return 10.0  # Default fallback    # Use recent data to estimate variability    recent_data = panel_df.tail(12)['value_mmcf'].values    if len(recent_data) < 2:        return 10.0    # Calculate simple MAE-like metric from recent changes    changes = np.abs(np.diff(recent_data))    historical_mae = np.mean(changes) if len(changes) > 0 else 10.0    # Ensure reasonable bounds    historical_mae = max(1.0, min(historical_mae, 50.0))    logger.info("Historical MAE calculated for prediction intervals", extra={        "historical_mae": float(historical_mae),        "data_points": len(recent_data)    })    return historical_maedef write_forecast_csv(    forecasts: np.ndarray,    pi_lower: np.ndarray,    pi_upper: np.ndarray,    panel_df: pd.DataFrame,    batch_meta: Any,    forecast_path: Path,    horizon: int = 12) -> None:    """    Write forecast results to CSV file.    Args:        forecasts: Forecast values        pi_lower: Lower prediction interval bounds        pi_upper: Upper prediction interval bounds        panel_df: Panel DataFrame for date reference        batch_meta: Batch metadata        forecast_path: Path to forecast CSV file        horizon: Forecast horizon    """    # Get the last date from panel data    last_date = panel_df['period'].max()    # Generate future dates (month-end)    future_dates = []    current_date = pd.Timestamp(last_date)    for i in range(1, horizon + 1):        # Move to next month end        next_month = current_date + pd.DateOffset(months=i)        month_end = next_month.replace(day=1) + pd.DateOffset(months=1) - pd.DateOffset(days=1)        future_dates.append(month_end.date())    # Create forecast DataFrame    forecast_df = pd.DataFrame({        'period': future_dates,        'forecast': forecasts,        'pi_lo': pi_lower,        'pi_hi': pi_upper,        'batch_id': str(batch_meta.batch_id),        'asof_ts': batch_meta.asof_ts.isoformat()    })    # Ensure directory exists    forecast_path.parent.mkdir(parents=True, exist_ok=True)    # Write to CSV    forecast_df.to_csv(forecast_path, index=False)    logger.info("Forecast CSV written successfully", extra={        "path": str(forecast_path),        "rows": len(forecast_df),        "date_range": f"{forecast_df['period'].min()} to {forecast_df['period'].max()}"    })def update_model_card(    model_card_path: Path,    batch_meta: Any,    model: str,    forecast_stats: Dict[str, Any]) -> None:    """    Update the living model card with latest forecast information.    Args:        model_card_path: Path to MODEL_CARD.md        batch_meta: Batch metadata        model: Winning model name        forecast_stats: Forecast statistics    """    # Read existing model card if it exists    if model_card_path.exists():        with open(model_card_path, 'r', encoding='utf-8') as f:            content = f.read()    else:        content = _create_default_model_card()    # Update the model card with latest information    updated_content = _update_model_card_content(        content, batch_meta, model, forecast_stats    )    # Write updated content    with open(model_card_path, 'w', encoding='utf-8') as f:        f.write(updated_content)    logger.info("Model card updated successfully", extra={        "path": str(model_card_path),        "model": model,        "batch_id": str(batch_meta.batch_id)    })def _create_default_model_card() -> str:    """Create default model card content."""    return """# Fuel Tracker Model Card## Model Information- **Model Name**: Fuel Consumption Forecasting- **Version**: 0.1.0- **Last Updated**: [Auto-updated]- **Batch ID**: [Auto-updated]## ObjectiveForecast monthly pipeline compressor fuel consumption in million cubic feet (MMcf) to support operational planning and resource allocation.## Target Variable- **Variable**: `value_mmcf`- **Unit**: Million cubic feet (MMcf)- **Frequency**: Monthly- **Type**: Continuous, non-negative## Data Sources- **Primary**: EIA (Energy Information Administration) API v2- **Metric**: Pipeline compressor fuel consumption- **Coverage**: United States- **Update Frequency**: Monthly (with provisional data handling)## Lineage Rules- **Data Freshness**: Cache TTL of 3 business days- **Provisional Mode**: Blocks publishing when cache is stale or API fails- **Batch Tracking**: Each data update creates a new batch with UUID and timestamp- **Append-Only**: Panel data uses append-only revisions with lineage tracking## Model Selection- **Current Winner**: [Auto-updated]- **Selection Criteria**: [To be implemented]- **Backtest Performance**: [Auto-updated]## Forecast Details- **Horizon**: 12 months- **Frequency**: Monthly- **Prediction Intervals**: Naive bands using historical MAE- **Last Forecast**: [Auto-updated]## Tolerance & Performance- **Target MAE**: [To be defined]- **Target sMAPE**: [To be defined]- **Acceptable Range**: [To be defined]## Backtest Protocol- **Method**: Rolling backtest with frozen-origin style- **Horizon**: 12 months (configurable)- **Lookback**: 60 months (configurable)- **Metrics**: MAE, sMAPE, RMSE, MAPE- **Validation**: Time series cross-validation## Success Criteria- **Accuracy**: Forecasts within acceptable error bounds- **Stability**: Consistent performance across backtest periods- **Reliability**: Robust handling of data updates and model revisions## Limitations- **Seasonality**: Assumes stable seasonal patterns- **Trend**: Linear trend assumption may not capture complex dynamics- **Exogenous Variables**: Limited incorporation of external factors- **Data Quality**: Dependent on EIA data availability and accuracy## Revision Stability- **Log Flips**: Model performance changes tracked through batch lineage- **Version Control**: Each forecast run creates new batch with full traceability- **Rollback Capability**: Previous forecasts preserved in lineage log- **Impact Assessment**: Performance changes can be traced to specific data updates## Technical Details- **Framework**: Python with pandas, numpy- **Storage**: Parquet format with PyArrow- **Lineage**: JSONL logs with batch metadata- **Configuration**: Environment-based with .env files## Maintenance- **Regular Updates**: Monthly with new EIA data- **Performance Monitoring**: Continuous backtesting- **Model Refresh**: Quarterly model selection updates- **Documentation**: Living document updated with each forecast run"""def _update_model_card_content(    content: str,    batch_meta: Any,    model: str,    forecast_stats: Dict[str, Any]) -> str:    """    Update model card content with latest information.    Args:        content: Existing model card content        batch_meta: Batch metadata        model: Winning model name        forecast_stats: Forecast statistics    Returns:        Updated model card content    """    # Update timestamp    content = content.replace(        "**Last Updated**: [Auto-updated]",        f"**Last Updated**: {batch_meta.asof_ts.strftime('%Y-%m-%d %H:%M:%S UTC')}"    )    # Update batch ID    content = content.replace(        "**Batch ID**: [Auto-updated]",        f"**Batch ID**: {batch_meta.batch_id}"    )    # Update current winner    content = content.replace(        "**Current Winner**: [Auto-updated]",        f"**Current Winner**: {model}"    )    # Update last forecast    content = content.replace(        "**Last Forecast**: [Auto-updated]",        f"**Last Forecast**: {batch_meta.asof_ts.strftime('%Y-%m-%d %H:%M:%S UTC')}"    )    # Add forecast statistics if available    if forecast_stats:        stats_section = f"""## Latest Forecast Statistics- **Model Used**: {model}- **Forecast Horizon**: {forecast_stats.get('horizon', 'N/A')} months- **Forecast Mean**: {forecast_stats.get('forecast_mean', 'N/A'):.2f} MMcf- **PI Half-Width**: {forecast_stats.get('pi_half_width', 'N/A'):.2f} MMcf- **Generated**: {batch_meta.asof_ts.strftime('%Y-%m-%d %H:%M:%S UTC')}"""        # Remove all existing forecast statistics sections        while "## Latest Forecast Statistics" in content:            start_idx = content.find("## Latest Forecast Statistics")            next_section_idx = content.find("##", start_idx + 1)            if next_section_idx != -1:                content = content[:start_idx] + content[next_section_idx:]            else:                content = content[:start_idx]        # Insert new forecast statistics section after Forecast Details        if "## Forecast Details" in content:            content = content.replace(                "## Forecast Details",                "## Forecast Details" + stats_section            )    return contentdef run_forecast_pipeline(    model: Optional[str] = None,    horizon: int = 12) -> Dict[str, Any]:    """    Run complete forecast pipeline.    Args:        model: Model to use (if None, selects winning model)        horizon: Forecast horizon    Returns:        Dictionary with pipeline results    """    logger.info("Starting forecast pipeline", extra={        "model": model,        "horizon": horizon    })    # Start batch for this forecast run    batch = start_batch(source="FORECAST", notes=f"Forecast run for {model or 'auto-selected'} model")    try:        # Get panel path        panel_path = OUTPUTS_DIR / "panel_monthly.parquet"        # Read latest panel        panel_df = read_panel(panel_path)        if panel_df.empty:            return {                "error": "No panel data available for forecasting",                "batch_id": str(batch.batch_id)            }        # Select winning model if not specified        if model is None:            metrics_path = OUTPUTS_DIR / "metrics.csv"            model = select_winning_model(metrics_path)            logger.info(f"Selected winning model: {model}")        # Generate forecast with prediction intervals        forecasts, pi_lower, pi_upper = generate_forecast_with_pi(            panel_df, model, horizon        )        # Write forecast CSV        forecast_path = OUTPUTS_DIR / FORECAST_FILE        write_forecast_csv(            forecasts, pi_lower, pi_upper, panel_df, batch, forecast_path, horizon        )        # Update model card        model_card_path = Path("MODEL_CARD.md")        forecast_stats = {            'horizon': horizon,            'forecast_mean': float(np.mean(forecasts)),            'pi_half_width': float(np.mean(pi_upper - pi_lower) / 2)        }        update_model_card(model_card_path, batch, model, forecast_stats)        # Calculate summary statistics        results = {            "batch_id": str(batch.batch_id),            "asof_ts": batch.asof_ts.isoformat(),            "model": model,            "data_rows": len(panel_df),            "horizon": horizon,            "forecast_mean": float(np.mean(forecasts)),            "forecast_std": float(np.std(forecasts)),            "pi_half_width": float(np.mean(pi_upper - pi_lower) / 2),            "forecast_file": str(forecast_path),            "model_card_file": str(model_card_path)        }        logger.info("Forecast pipeline completed successfully", extra=results)        return results    except Exception as e:        logger.error("Forecast pipeline failed", extra={            "error": str(e),            "batch_id": str(batch.batch_id)        })        raisedef main():    """Main entry point for forecast pipeline."""    parser = argparse.ArgumentParser(description="Run forecast pipeline")    parser.add_argument(        "--model",        type=str,        choices=["baseline", "stl_ets", "sarimax"],        help="Model to use for forecasting (if not specified, selects winning model)"    )    parser.add_argument(        "--horizon",        type=int,        default=12,        help="Forecast horizon in months"    )    args = parser.parse_args()    try:        results = run_forecast_pipeline(            model=args.model,            horizon=args.horizon        )        if "error" in results:            print(f"❌ Forecast failed: {results['error']}")            sys.exit(1)        # Print results        print("\n🚀 Forecast Results:")        print(f"   Batch ID: {results['batch_id']}")        print(f"   Model: {results['model']}")        print(f"   Data rows: {results['data_rows']}")        print(f"   Horizon: {results['horizon']} months")        print(f"   Forecast mean: {results['forecast_mean']:.2f} MMcf")        print(f"   Forecast std: {results['forecast_std']:.2f} MMcf")        print(f"   PI half-width: {results['pi_half_width']:.2f} MMcf")        print(f"   Forecast file: {results['forecast_file']}")        print(f"   Model card: {results['model_card_file']}")        print("\n✅ Forecast completed successfully!")        sys.exit(0)    except Exception as e:        print(f"❌ Forecast failed: {e}")        sys.exit(1)if __name__ == "__main__":    main()